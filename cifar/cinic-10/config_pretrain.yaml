#sample_argument:
#type || default value
#value
#print_freq  int || 10  
#resume      bool || False     * resume from checkpoint
#gpu_id      int  || null      * gpu id to use
#epochs      int  || 200       * number of training epochs for chosen stage
#optimizer   str  || sgd       * optimizer for the chosen stage
#lr          float || 0.1      * learning rate
#lr_scheduler str || default,cosine  * learning rate scheduler
#arch        str  || resnet18  * architecture of model
#save_model  str  || saved_model.pth * filename to save model
#load_model  str  || loaded_model.pth  * filename to load model
#workers     int  || 16        * number of threads for loading images
#logging     bool || False     * whether to log files
#model_dir   str  || models      * destination folder to drop model file
#log_dir     str  || logs      * destination folder to drop log file
#smooth_eps  float|| 0.0       * smoothing rate [0.0, 1.0], set to 0.0 to disable
#alpha       float|| 0.0       * for mixup training, lambda = Beta(alpha, alpha) distribution. Set to 0.0 to disable
#                              * chosen value of alpha in Bag of Tricks is 0.2, and increase # of training epochs by 67%
#warmup_epochs      int  || 0       * number of epochs for lr warmup
#warmup_lr      float  || 0.0001       * initial lr before warmup



general:
  random_seed:
    1
  print_freq:
    50
  resume:  
    False 
  gpu_id:  
    0
  workers: 
    8
  arch: 
    cifar10_cnn
  width_multiplier:
    default
  model_dir: 
    models
  log_dir: 
    logs
  init_func:
    default
  smooth_eps:
    0.0
  alpha:
    0.0
pretrain:
  epochs: 
    200
  optimizer:
    sgd
    # adam
  lr:
    0.1
    # 1e-3
  lr_scheduler: 
    cosine
  save_model: 
    # cinic10_cnn.pth
    cinic10_cnn_s.pth
  auxiliary:
    False
    # True
validate:
  load_model: 
    # cifar10_cnn.pth
    # cifar10_aux.pth
    # cifar10_adv.pth
    cifar10_dp.pth
  split_index:
    14
  noise_scale:
    5.0
  dcor_weighting:
    0.0
inversion:
  epochs: 
    50
  optimizer:
    adam
  lr:  
    0.001
  lr_scheduler: 
    default
  save_model: 
    # cifar10_inv_16*16.pth
    # cifar10_aux_inv.pth
    # cifar10_adv_inv.pth
    # cifar10_dp_inv.pth
    cifar10_inv_adv_0.2_16*16.pth
  load_model: 
    # cifar10_cnn.pth
    # cifar10_aux.pth
    # cifar10_adv.pth
    # cifar10_dp.pth
    cifar10_adv_0.2_16*16.pth
  layers:
    ['features.6']
    # ['noise_layer']
  split_index:
    7
  noise_scale:
    5.0
  plot:
    True
advtrain:
  epochs:
    50
  ckpt_interval:
    10
  optimizer: 
    adam
  lr:
    0.001
  lr_scheduler:
    default
  save_model: 
    cifar10_adv_0.2_16*16.pth
  load_model:
    cifar10_cnn.pth
  layers:
    ['features.6']
  inv_lr:
    0.001
  num_steps:
    1
  gamma:
    0.2
  load_inv_model:
    cifar10_inv_16*16.pth
noisytrain:
  epochs:
    50
  optimizer: 
    adam
  lr:
    0.001
  lr_scheduler:
    default
  save_model: 
    # cifar10_dp.pth
    cifar10_dp_8_16*16.pth
  split_index:
    7
  noise_scale:
    8.0
  dcor_weighting:
    0.0
antitrain:
  epochs:
    50
  optimizer: 
    adam
  lr:
    0.001
  lr_scheduler:
    default
  save_model: 
    cifar10_anti-adv.pth
    # cifar10_anti-dp.pth
  load_model:
    cifar10_adv.pth
    # cifar10_dp.pth
  load_aux_model:
    cifar10_aux.pth
  layers:
    ['features.13']
    # ['noise_layer']
  aux_layers:
    ['features.13']
  # split_index:
  #   14
  # noise_scale:
  #   5.0
  inv_lr:
    0.001
  beta:
    0.0
  plot:
    True
